{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c017511",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37bf145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, datasets\n",
    "from torch.autograd import Variable\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6f660",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4c730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.cuda.manual_seed(1337)\n",
    "\n",
    "batch_size = 100\n",
    "test_batch_size = 1000\n",
    "gamma = 0.001\n",
    "lr = 0.01\n",
    "prune_rate=0.4\n",
    "\n",
    "kwargs = {'num_workers': 16, 'pin_memory': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a92f1",
   "metadata": {},
   "source": [
    "DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58c5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Pad(4),\n",
    "                       transforms.RandomCrop(32),\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bbe0f",
   "metadata": {},
   "source": [
    "Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a11852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequential_model(nn.Module):\n",
    "    def __init__(self, layers=None):\n",
    "        super(sequential_model, self).__init__()\n",
    "        if layers == None:\n",
    "            layers = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512]\n",
    "        num_classes = 10\n",
    "        self.feature = self.make_layers(layers)\n",
    "        self.classifier = nn.Linear(layers[-1], num_classes)\n",
    "    \n",
    "    def make_layers(self, structure):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in structure:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = nn.AvgPool2d(2)(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y = self.classifier(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec86493",
   "metadata": {},
   "source": [
    "Train Epoch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf83829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer, data_loader=train_loader):\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        sum_channel_scaling_factors = 0\n",
    "        \n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                sum_channel_scaling_factors += torch.sum(m.weight.data.abs())\n",
    "        \n",
    "        loss = F.cross_entropy(output, target) + gamma * sum_channel_scaling_factors\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, idx * len(data), len(data_loader.dataset),\n",
    "            100. * idx / len(data_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7b31e",
   "metadata": {},
   "source": [
    "Validation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeeddf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader=test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in data_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)        \n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).data.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        test_loss /= len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "    return correct / float(len(data_loader.dataset))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc0761",
   "metadata": {},
   "source": [
    "Save Model Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6656403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint_sr.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best_sr.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f892aa61",
   "metadata": {},
   "source": [
    "Train network method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc841441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10):\n",
    "    \n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_prec = 0.\n",
    "    for i in range(0, epochs):\n",
    "        train(model, i, optimizer)\n",
    "        prec = test(model)\n",
    "        is_best = prec > best_prec\n",
    "        best_prec1 = max(prec, best_prec)\n",
    "        save_checkpoint({\n",
    "            'epoch': i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db21bd4",
   "metadata": {},
   "source": [
    "Load existing Model method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec19a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path=\"checkpoint_sr.pth.tar\", model_path=\"model_best_sr.pth.tar\"):\n",
    "    model = sequential_model()\n",
    "    model.cuda()\n",
    "    if os.path.isfile(model_path):\n",
    "        print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "        checkpoint_path = torch.load(model_path)\n",
    "        best_prec1 = checkpoint_path['best_prec1']\n",
    "        model.load_state_dict(checkpoint_path['state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {}) Prec1: {:f}\"\n",
    "              .format(model, checkpoint_path['epoch'], best_prec1))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2690475",
   "metadata": {},
   "source": [
    "Select weak channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c609836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectChannels(model, percent=0.2):\n",
    "    total = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            total += m.weight.data.shape[0]\n",
    "\n",
    "    bn = torch.zeros(total)\n",
    "    index = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            size = m.weight.data.shape[0]\n",
    "            bn[index:(index+size)] = m.weight.data.abs().clone()\n",
    "            index += size\n",
    "\n",
    "    y, i = torch.sort(bn)\n",
    "    thre_index = int(total * percent)\n",
    "    thre = y[thre_index]\n",
    "\n",
    "    pruned = 0\n",
    "    cfg = []\n",
    "    cfg_mask = []\n",
    "    for k, m in enumerate(model.modules()):\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            weight_copy = m.weight.data.clone()\n",
    "            print(type(weight_copy.abs().gt(thre).float()))\n",
    "            #mask is a matrix in which 1 marks the channels which are kept and 0 marks the pruned channels\n",
    "            mask = weight_copy.abs().gt(thre).float().cuda()          \n",
    "            #pruned is the number of all pruned channels \n",
    "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "            cfg.append(int(torch.sum(mask)))\n",
    "            cfg_mask.append(mask.clone())\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.\n",
    "                format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "        elif isinstance(m, nn.MaxPool2d):\n",
    "            cfg.append('M')\n",
    "    return cfg, cfg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a1f710",
   "metadata": {},
   "source": [
    "Build new model and transfer weights from full model to build the new pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c09ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_params(cfg, cfg_mask, model):\n",
    "    newmodel = sequential_model(layers=cfg)\n",
    "    newmodel.cuda() \n",
    "\n",
    "    layer_id_in_cfg = 0\n",
    "    start_mask = torch.ones(3)\n",
    "    end_mask = cfg_mask[layer_id_in_cfg]\n",
    "    for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
    "        if isinstance(m0, nn.BatchNorm2d):\n",
    "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
    "            m1.weight.data = m0.weight.data[idx1].clone()\n",
    "            m1.bias.data = m0.bias.data[idx1].clone()\n",
    "            m1.running_mean = m0.running_mean[idx1].clone()\n",
    "            m1.running_var = m0.running_var[idx1].clone()\n",
    "            layer_id_in_cfg += 1\n",
    "            start_mask = end_mask.clone()\n",
    "            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
    "                end_mask = cfg_mask[layer_id_in_cfg]\n",
    "        elif isinstance(m0, nn.Conv2d):\n",
    "            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
    "            print('In shape: {:d} Out shape:{:d}'.format(idx0.shape[0], idx1.shape[0]))\n",
    "            w = m0.weight.data[:, idx0, :, :].clone()\n",
    "            w = w[idx1, :, :, :].clone()\n",
    "            m1.weight.data = w.clone()\n",
    "            # m1.bias.data = m0.bias.data[idx1].clone()\n",
    "        elif isinstance(m0, nn.Linear):\n",
    "            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "            m1.weight.data = m0.weight.data[:, idx0].clone()    \n",
    "    return newmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38e5f8",
   "metadata": {},
   "source": [
    "Prune trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64546cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, percent=0.3):\n",
    "    cfg, cfg_mask = selectChannels(model, percent)\n",
    "    prune_model = transfer_params(cfg, cfg_mask, model)\n",
    "    torch.save({'cfg': cfg, 'state_dict': prune_model.state_dict()}, f='pruned_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664c7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 5.247394\n",
      "Train Epoch: 0 [10000/50000 (20.0%)]\tLoss: 4.880222\n",
      "Train Epoch: 0 [20000/50000 (40.0%)]\tLoss: 4.538634\n",
      "Train Epoch: 0 [30000/50000 (60.0%)]\tLoss: 4.469398\n",
      "Train Epoch: 0 [40000/50000 (80.0%)]\tLoss: 4.243611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddy/Programme/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1482, Accuracy: 4467/10000 (44.7%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 4.224677\n",
      "Train Epoch: 1 [10000/50000 (20.0%)]\tLoss: 4.174125\n",
      "Train Epoch: 1 [20000/50000 (40.0%)]\tLoss: 4.074605\n",
      "Train Epoch: 1 [30000/50000 (60.0%)]\tLoss: 3.763494\n",
      "Train Epoch: 1 [40000/50000 (80.0%)]\tLoss: 3.728575\n",
      "\n",
      "Test set: Average loss: 0.1024, Accuracy: 6415/10000 (64.2%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 3.682876\n",
      "Train Epoch: 2 [10000/50000 (20.0%)]\tLoss: 3.560416\n",
      "Train Epoch: 2 [20000/50000 (40.0%)]\tLoss: 3.717249\n",
      "Train Epoch: 2 [30000/50000 (60.0%)]\tLoss: 3.666487\n",
      "Train Epoch: 2 [40000/50000 (80.0%)]\tLoss: 3.554167\n",
      "\n",
      "Test set: Average loss: 0.0846, Accuracy: 7130/10000 (71.3%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 3.320322\n",
      "Train Epoch: 3 [10000/50000 (20.0%)]\tLoss: 3.440720\n",
      "Train Epoch: 3 [20000/50000 (40.0%)]\tLoss: 3.489455\n",
      "Train Epoch: 3 [30000/50000 (60.0%)]\tLoss: 3.331817\n",
      "Train Epoch: 3 [40000/50000 (80.0%)]\tLoss: 3.363400\n",
      "\n",
      "Test set: Average loss: 0.0778, Accuracy: 7452/10000 (74.5%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 3.304054\n",
      "Train Epoch: 4 [10000/50000 (20.0%)]\tLoss: 3.412824\n",
      "Train Epoch: 4 [20000/50000 (40.0%)]\tLoss: 3.273837\n",
      "Train Epoch: 4 [30000/50000 (60.0%)]\tLoss: 3.174760\n",
      "Train Epoch: 4 [40000/50000 (80.0%)]\tLoss: 3.356662\n",
      "\n",
      "Test set: Average loss: 0.0647, Accuracy: 7693/10000 (76.9%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 3.222014\n",
      "Train Epoch: 5 [10000/50000 (20.0%)]\tLoss: 3.198694\n",
      "Train Epoch: 5 [20000/50000 (40.0%)]\tLoss: 3.266375\n",
      "Train Epoch: 5 [30000/50000 (60.0%)]\tLoss: 3.336300\n",
      "Train Epoch: 5 [40000/50000 (80.0%)]\tLoss: 3.206342\n",
      "\n",
      "Test set: Average loss: 0.0709, Accuracy: 7666/10000 (76.7%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 3.274774\n",
      "Train Epoch: 6 [10000/50000 (20.0%)]\tLoss: 3.082029\n",
      "Train Epoch: 6 [20000/50000 (40.0%)]\tLoss: 3.294711\n",
      "Train Epoch: 6 [30000/50000 (60.0%)]\tLoss: 3.113334\n",
      "Train Epoch: 6 [40000/50000 (80.0%)]\tLoss: 3.167029\n",
      "\n",
      "Test set: Average loss: 0.0563, Accuracy: 8124/10000 (81.2%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 3.177619\n",
      "Train Epoch: 7 [10000/50000 (20.0%)]\tLoss: 3.191283\n",
      "Train Epoch: 7 [20000/50000 (40.0%)]\tLoss: 3.046049\n",
      "Train Epoch: 7 [30000/50000 (60.0%)]\tLoss: 3.215760\n",
      "Train Epoch: 7 [40000/50000 (80.0%)]\tLoss: 3.075698\n",
      "\n",
      "Test set: Average loss: 0.0508, Accuracy: 8261/10000 (82.6%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 3.079846\n",
      "Train Epoch: 8 [10000/50000 (20.0%)]\tLoss: 3.120825\n",
      "Train Epoch: 8 [20000/50000 (40.0%)]\tLoss: 3.237451\n",
      "Train Epoch: 8 [30000/50000 (60.0%)]\tLoss: 3.069663\n",
      "Train Epoch: 8 [40000/50000 (80.0%)]\tLoss: 3.143550\n",
      "\n",
      "Test set: Average loss: 0.0492, Accuracy: 8381/10000 (83.8%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 3.070486\n",
      "Train Epoch: 9 [10000/50000 (20.0%)]\tLoss: 3.203788\n",
      "Train Epoch: 9 [20000/50000 (40.0%)]\tLoss: 3.169047\n"
     ]
    }
   ],
   "source": [
    "model = train_model(sequential_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_model(model, prune_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "safed = torch.load('pruned_model.pt')\n",
    "structure = safed['cfg']\n",
    "weights = safed['state_dict']\n",
    "pruned_model = sequential_model(structure)\n",
    "pruned_model.load_state_dict(weights)\n",
    "pruned_model.cuda()\n",
    "test(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = train_model(pruned_model, epochs=2)\n",
    "test(fine_tuned_model)\n",
    "print('Number of parameters before pruning: ' + str(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "print('Number of parameters after pruning: ' + str(sum(p.numel() for p in pruned_model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3b014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2420286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0077c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
