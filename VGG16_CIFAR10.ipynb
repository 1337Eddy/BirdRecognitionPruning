{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c017511",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37bf145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, datasets\n",
    "from torch.autograd import Variable\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6f660",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4c730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.cuda.manual_seed(1337)\n",
    "\n",
    "batch_size = 100\n",
    "test_batch_size = 1000\n",
    "gamma = 0.001\n",
    "lr = 0.01\n",
    "prune_rate=0.4\n",
    "\n",
    "kwargs = {'num_workers': 16, 'pin_memory': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a92f1",
   "metadata": {},
   "source": [
    "DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58c5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Pad(4),\n",
    "                       transforms.RandomCrop(32),\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bbe0f",
   "metadata": {},
   "source": [
    "Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a11852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequential_model(nn.Module):\n",
    "    def __init__(self, layers=None):\n",
    "        super(sequential_model, self).__init__()\n",
    "        if layers == None:\n",
    "            layers = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512]\n",
    "        num_classes = 10\n",
    "        self.feature = self.make_layers(layers)\n",
    "        self.classifier = nn.Linear(layers[-1], num_classes)\n",
    "    \n",
    "    def make_layers(self, structure):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in structure:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = nn.AvgPool2d(2)(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y = self.classifier(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec86493",
   "metadata": {},
   "source": [
    "Train Epoch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf83829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer, data_loader=train_loader):\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        sum_channel_scaling_factors = 0\n",
    "        \n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                sum_channel_scaling_factors += torch.sum(m.weight.data.abs())\n",
    "        \n",
    "        loss = F.cross_entropy(output, target) + gamma * sum_channel_scaling_factors\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, idx * len(data), len(data_loader.dataset),\n",
    "            100. * idx / len(data_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7b31e",
   "metadata": {},
   "source": [
    "Validation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeeddf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader=test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in data_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)        \n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).data.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        test_loss /= len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "    return correct / float(len(data_loader.dataset))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc0761",
   "metadata": {},
   "source": [
    "Save Model Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6656403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint_sr.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best_sr.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f892aa61",
   "metadata": {},
   "source": [
    "Train network method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc841441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10):\n",
    "    \n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_prec = 0.\n",
    "    for i in range(0, epochs):\n",
    "        train(model, i, optimizer)\n",
    "        prec = test(model)\n",
    "        is_best = prec > best_prec\n",
    "        best_prec1 = max(prec, best_prec)\n",
    "        save_checkpoint({\n",
    "            'epoch': i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db21bd4",
   "metadata": {},
   "source": [
    "Load existing Model method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec19a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path=\"checkpoint_sr.pth.tar\", model_path=\"model_best_sr.pth.tar\"):\n",
    "    model = sequential_model()\n",
    "    model.cuda()\n",
    "    if os.path.isfile(model_path):\n",
    "        print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "        checkpoint_path = torch.load(model_path)\n",
    "        best_prec1 = checkpoint_path['best_prec1']\n",
    "        model.load_state_dict(checkpoint_path['state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {}) Prec1: {:f}\"\n",
    "              .format(model, checkpoint_path['epoch'], best_prec1))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2690475",
   "metadata": {},
   "source": [
    "Select weak channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c609836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectChannels(model, percent=0.2):\n",
    "    total = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            total += m.weight.data.shape[0]\n",
    "\n",
    "    bn = torch.zeros(total)\n",
    "    index = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            size = m.weight.data.shape[0]\n",
    "            bn[index:(index+size)] = m.weight.data.abs().clone()\n",
    "            index += size\n",
    "\n",
    "    y, i = torch.sort(bn)\n",
    "    thre_index = int(total * percent)\n",
    "    thre = y[thre_index]\n",
    "\n",
    "    pruned = 0\n",
    "    cfg = []\n",
    "    cfg_mask = []\n",
    "    for k, m in enumerate(model.modules()):\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            weight_copy = m.weight.data.clone()\n",
    "            print(type(weight_copy.abs().gt(thre).float()))\n",
    "            #mask is a matrix in which 1 marks the channels which are kept and 0 marks the pruned channels\n",
    "            mask = weight_copy.abs().gt(thre).float().cuda()          \n",
    "            #pruned is the number of all pruned channels \n",
    "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "            cfg.append(int(torch.sum(mask)))\n",
    "            cfg_mask.append(mask.clone())\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.\n",
    "                format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "        elif isinstance(m, nn.MaxPool2d):\n",
    "            cfg.append('M')\n",
    "    return cfg, cfg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a1f710",
   "metadata": {},
   "source": [
    "Build new model and transfer weights from full model to build the new pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c09ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_params(cfg, cfg_mask, model):\n",
    "    newmodel = sequential_model(layers=cfg)\n",
    "    newmodel.cuda() \n",
    "\n",
    "    layer_id_in_cfg = 0\n",
    "    start_mask = torch.ones(3)\n",
    "    end_mask = cfg_mask[layer_id_in_cfg]\n",
    "    for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
    "        if isinstance(m0, nn.BatchNorm2d):\n",
    "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
    "            m1.weight.data = m0.weight.data[idx1].clone()\n",
    "            m1.bias.data = m0.bias.data[idx1].clone()\n",
    "            m1.running_mean = m0.running_mean[idx1].clone()\n",
    "            m1.running_var = m0.running_var[idx1].clone()\n",
    "            layer_id_in_cfg += 1\n",
    "            start_mask = end_mask.clone()\n",
    "            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
    "                end_mask = cfg_mask[layer_id_in_cfg]\n",
    "        elif isinstance(m0, nn.Conv2d):\n",
    "            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
    "            print('In shape: {:d} Out shape:{:d}'.format(idx0.shape[0], idx1.shape[0]))\n",
    "            w = m0.weight.data[:, idx0, :, :].clone()\n",
    "            w = w[idx1, :, :, :].clone()\n",
    "            m1.weight.data = w.clone()\n",
    "            # m1.bias.data = m0.bias.data[idx1].clone()\n",
    "        elif isinstance(m0, nn.Linear):\n",
    "            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "            m1.weight.data = m0.weight.data[:, idx0].clone()    \n",
    "    return newmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38e5f8",
   "metadata": {},
   "source": [
    "Prune trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64546cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, percent=0.3):\n",
    "    cfg, cfg_mask = selectChannels(model, percent)\n",
    "    prune_model = transfer_params(cfg, cfg_mask, model)\n",
    "    torch.save({'cfg': cfg, 'state_dict': prune_model.state_dict()}, f='pruned_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c664c7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 5.294731\n",
      "Train Epoch: 0 [10000/50000 (20.0%)]\tLoss: 4.919122\n",
      "Train Epoch: 0 [20000/50000 (40.0%)]\tLoss: 4.569302\n",
      "Train Epoch: 0 [30000/50000 (60.0%)]\tLoss: 4.330019\n",
      "Train Epoch: 0 [40000/50000 (80.0%)]\tLoss: 4.334615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddy/Programme/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1519, Accuracy: 4345/10000 (43.5%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 4.021239\n",
      "Train Epoch: 1 [10000/50000 (20.0%)]\tLoss: 4.156187\n",
      "Train Epoch: 1 [20000/50000 (40.0%)]\tLoss: 3.734669\n",
      "Train Epoch: 1 [30000/50000 (60.0%)]\tLoss: 4.070164\n",
      "Train Epoch: 1 [40000/50000 (80.0%)]\tLoss: 3.734694\n",
      "\n",
      "Test set: Average loss: 0.1201, Accuracy: 5750/10000 (57.5%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 3.852301\n",
      "Train Epoch: 2 [10000/50000 (20.0%)]\tLoss: 3.541624\n",
      "Train Epoch: 2 [20000/50000 (40.0%)]\tLoss: 3.486603\n",
      "Train Epoch: 2 [30000/50000 (60.0%)]\tLoss: 3.551894\n",
      "Train Epoch: 2 [40000/50000 (80.0%)]\tLoss: 3.620374\n",
      "\n",
      "Test set: Average loss: 0.0862, Accuracy: 7060/10000 (70.6%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 3.374844\n",
      "Train Epoch: 3 [10000/50000 (20.0%)]\tLoss: 3.414492\n",
      "Train Epoch: 3 [20000/50000 (40.0%)]\tLoss: 3.391203\n",
      "Train Epoch: 3 [30000/50000 (60.0%)]\tLoss: 3.608839\n",
      "Train Epoch: 3 [40000/50000 (80.0%)]\tLoss: 3.363187\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 7558/10000 (75.6%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 3.392163\n",
      "Train Epoch: 4 [10000/50000 (20.0%)]\tLoss: 3.164254\n",
      "Train Epoch: 4 [20000/50000 (40.0%)]\tLoss: 3.238097\n",
      "Train Epoch: 4 [30000/50000 (60.0%)]\tLoss: 3.126158\n",
      "Train Epoch: 4 [40000/50000 (80.0%)]\tLoss: 3.490492\n",
      "\n",
      "Test set: Average loss: 0.0602, Accuracy: 7955/10000 (79.6%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 3.176398\n",
      "Train Epoch: 5 [10000/50000 (20.0%)]\tLoss: 3.277431\n",
      "Train Epoch: 5 [20000/50000 (40.0%)]\tLoss: 3.334768\n",
      "Train Epoch: 5 [30000/50000 (60.0%)]\tLoss: 3.176779\n",
      "Train Epoch: 5 [40000/50000 (80.0%)]\tLoss: 3.221533\n",
      "\n",
      "Test set: Average loss: 0.0651, Accuracy: 8062/10000 (80.6%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 3.167408\n",
      "Train Epoch: 6 [10000/50000 (20.0%)]\tLoss: 3.124903\n",
      "Train Epoch: 6 [20000/50000 (40.0%)]\tLoss: 3.342604\n",
      "Train Epoch: 6 [30000/50000 (60.0%)]\tLoss: 3.338655\n",
      "Train Epoch: 6 [40000/50000 (80.0%)]\tLoss: 3.204226\n",
      "\n",
      "Test set: Average loss: 0.0462, Accuracy: 8233/10000 (82.3%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 3.156481\n",
      "Train Epoch: 7 [10000/50000 (20.0%)]\tLoss: 3.124857\n",
      "Train Epoch: 7 [20000/50000 (40.0%)]\tLoss: 3.180586\n",
      "Train Epoch: 7 [30000/50000 (60.0%)]\tLoss: 3.147736\n",
      "Train Epoch: 7 [40000/50000 (80.0%)]\tLoss: 3.188131\n",
      "\n",
      "Test set: Average loss: 0.0568, Accuracy: 8282/10000 (82.8%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 3.239935\n",
      "Train Epoch: 8 [10000/50000 (20.0%)]\tLoss: 3.160404\n",
      "Train Epoch: 8 [20000/50000 (40.0%)]\tLoss: 3.063294\n",
      "Train Epoch: 8 [30000/50000 (60.0%)]\tLoss: 3.129011\n",
      "Train Epoch: 8 [40000/50000 (80.0%)]\tLoss: 3.107381\n",
      "\n",
      "Test set: Average loss: 0.0749, Accuracy: 7803/10000 (78.0%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 3.148158\n",
      "Train Epoch: 9 [10000/50000 (20.0%)]\tLoss: 3.202374\n",
      "Train Epoch: 9 [20000/50000 (40.0%)]\tLoss: 3.066881\n",
      "Train Epoch: 9 [30000/50000 (60.0%)]\tLoss: 3.118174\n",
      "Train Epoch: 9 [40000/50000 (80.0%)]\tLoss: 3.076761\n",
      "\n",
      "Test set: Average loss: 0.0555, Accuracy: 8347/10000 (83.5%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model(sequential_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e9f76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "layer index: 3 \t total channel: 64 \t remaining channel: 32\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 6 \t total channel: 64 \t remaining channel: 53\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 10 \t total channel: 128 \t remaining channel: 120\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 13 \t total channel: 128 \t remaining channel: 99\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 17 \t total channel: 256 \t remaining channel: 216\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 20 \t total channel: 256 \t remaining channel: 182\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 24 \t total channel: 512 \t remaining channel: 371\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 27 \t total channel: 512 \t remaining channel: 356\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 31 \t total channel: 512 \t remaining channel: 297\n",
      "<class 'torch.Tensor'>\n",
      "layer index: 34 \t total channel: 512 \t remaining channel: 40\n",
      "In shape: 3 Out shape:32\n",
      "In shape: 32 Out shape:53\n",
      "In shape: 53 Out shape:120\n",
      "In shape: 120 Out shape:99\n",
      "In shape: 99 Out shape:216\n",
      "In shape: 216 Out shape:182\n",
      "In shape: 182 Out shape:371\n",
      "In shape: 371 Out shape:356\n",
      "In shape: 356 Out shape:297\n",
      "In shape: 297 Out shape:40\n"
     ]
    }
   ],
   "source": [
    "prune_model(model, prune_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "854e4a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1496, Accuracy: 6090/10000 (60.9%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6090)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safed = torch.load('pruned_model.pt')\n",
    "structure = safed['cfg']\n",
    "weights = safed['state_dict']\n",
    "pruned_model = sequential_model(structure)\n",
    "pruned_model.load_state_dict(weights)\n",
    "pruned_model.cuda()\n",
    "test(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f0b0924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 2.711568\n",
      "Train Epoch: 0 [10000/50000 (20.0%)]\tLoss: 2.743242\n",
      "Train Epoch: 0 [20000/50000 (40.0%)]\tLoss: 2.501064\n",
      "Train Epoch: 0 [30000/50000 (60.0%)]\tLoss: 2.649864\n",
      "Train Epoch: 0 [40000/50000 (80.0%)]\tLoss: 2.551858\n",
      "\n",
      "Test set: Average loss: 0.0425, Accuracy: 8599/10000 (86.0%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 2.601574\n",
      "Train Epoch: 1 [10000/50000 (20.0%)]\tLoss: 2.601640\n",
      "Train Epoch: 1 [20000/50000 (40.0%)]\tLoss: 2.565262\n",
      "Train Epoch: 1 [30000/50000 (60.0%)]\tLoss: 2.757523\n",
      "Train Epoch: 1 [40000/50000 (80.0%)]\tLoss: 2.587004\n",
      "\n",
      "Test set: Average loss: 0.0438, Accuracy: 8715/10000 (87.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0441, Accuracy: 8715/10000 (87.2%)\n",
      "\n",
      "Number of parameters before pruning: 9413066\n",
      "Number of parameters after pruning: 3585384\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = train_model(pruned_model, epochs=2)\n",
    "test(fine_tuned_model)\n",
    "print('Number of parameters before pruning: ' + str(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "print('Number of parameters after pruning: ' + str(sum(p.numel() for p in pruned_model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3b014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2420286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1a42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
